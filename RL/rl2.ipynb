{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b987e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70aafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class MyFrozenLakeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.map = [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FHFFHFFF\",\n",
    "            \"FHFFFFHF\",\n",
    "            \"FFFFFFHF\",\n",
    "            \"FFFHFFFG\"\n",
    "        ]\n",
    "        self.nrow = len(self.map)\n",
    "        self.ncol = len(self.map[0])\n",
    "        self.nS = self.nrow * self.ncol\n",
    "        self.nA = 4\n",
    "        self.render_mode = render_mode\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.pos_to_state = lambda r, c: r * self.ncol + c\n",
    "        self.state_to_pos = lambda s: (s // self.ncol, s % self.ncol)\n",
    "        self.state = None\n",
    "        self.P = self._build_transition_matrix()\n",
    "\n",
    "    def _build_transition_matrix(self):\n",
    "        P = {s: {a: [] for a in range(self.nA)} for s in range(self.nS)}\n",
    "        for r in range(self.nrow):\n",
    "            for c in range(self.ncol):\n",
    "                s = self.pos_to_state(r, c)\n",
    "                tile = self.map[r][c]\n",
    "                for a in range(self.nA):\n",
    "                    if tile in \"GH\":\n",
    "                        P[s][a] = [(1.0, s, 0, True)]\n",
    "                        continue\n",
    "                    new_r, new_c = r, c\n",
    "                    if a == 0: new_c = max(c - 1, 0)\n",
    "                    elif a == 1: new_r = min(r + 1, self.nrow - 1)\n",
    "                    elif a == 2: new_c = min(c + 1, self.ncol - 1)\n",
    "                    elif a == 3: new_r = max(r - 1, 0)\n",
    "                    new_s = self.pos_to_state(new_r, new_c)\n",
    "                    new_tile = self.map[new_r][new_c]\n",
    "                    reward = 1 if new_tile == \"G\" else 0\n",
    "                    done = new_tile in \"GH\"\n",
    "                    P[s][a].append((1.0, new_s, reward, done))\n",
    "        return P\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = (0, 0)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        r, c = self.state\n",
    "        if action == 0: c = max(c - 1, 0)\n",
    "        elif action == 1: r = min(r + 1, self.nrow - 1)\n",
    "        elif action == 2: c = min(c + 1, self.ncol - 1)\n",
    "        elif action == 3: r = max(r - 1, 0)\n",
    "        self.state = (r, c)\n",
    "        tile = self.map[r][c]\n",
    "        reward = 1 if tile == \"G\" else 0\n",
    "        done = tile in \"GH\"\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r, c = self.state\n",
    "        return self.pos_to_state(r, c)\n",
    "\n",
    "    def render(self):\n",
    "        r, c = self.state\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                if (i, j) == (r, c):\n",
    "                    print(\" A \", end=\"\")\n",
    "                else:\n",
    "                    print(f\" {self.map[i][j]} \", end=\"\")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901d1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            A = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    A[a] += prob * (reward + gamma * V[next_state])\n",
    "            max_val = np.max(A)\n",
    "            delta = max(delta, abs(V[s] - max_val))\n",
    "            V[s] = max_val\n",
    "        if delta < theta: break\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                A[a] += prob * (reward + gamma * V[next_state])\n",
    "        policy[s] = np.argmax(A)\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f13af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-8):\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    V = np.zeros(env.nS)\n",
    "    stable = False\n",
    "    iterations = 0\n",
    "    while not stable:\n",
    "        iterations += 1\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.nS):\n",
    "                a = policy[s]\n",
    "                val = 0\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    val += prob * (reward + gamma * V[next_state])\n",
    "                delta = max(delta, abs(V[s] - val))\n",
    "                V[s] = val\n",
    "            if delta < theta: break\n",
    "        stable = True\n",
    "        for s in range(env.nS):\n",
    "            old = policy[s]\n",
    "            A = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    A[a] += prob * (reward + gamma * V[next_state])\n",
    "            new = np.argmax(A)\n",
    "            policy[s] = new\n",
    "            if new != old:\n",
    "                stable = False\n",
    "    return policy, V, iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd9e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy_arrows(policy, env):\n",
    "    arrows = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "    for r in range(env.nrow):\n",
    "        line = \"\"\n",
    "        for c in range(env.ncol):\n",
    "            s = env.pos_to_state(r, c)\n",
    "            ch = env.map[r][c]\n",
    "            if ch == \"H\":\n",
    "                line += \" H  \"\n",
    "            elif ch == \"G\":\n",
    "                line += \" G  \"\n",
    "            else:\n",
    "                line += f\" {arrows[policy[s]]}  \"\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a313db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "Success Rate: 1.0\n",
      "Avg Reward: 1.0\n",
      " ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓  \n",
      " ↓   ↓   ↓   →   ↓   ↓   ↓   ↓  \n",
      " ↓   ↓   ↓   H   →   →   ↓   ↓  \n",
      " ↓   →   ↓   ↓   ←   H   ↓   ↓  \n",
      " ↓   H   ↓   ↓   H   ↓   →   ↓  \n",
      " ↓   H   ↓   ↓   ↓   ↓   H   ↓  \n",
      " →   →   →   →   ↓   ↓   H   ↓  \n",
      " →   →   ↑   H   →   →   →   G  \n",
      "Policy Iteration\n",
      "Success Rate: 1.0\n",
      "Avg Reward: 1.0\n",
      "Iterations: 15\n",
      " ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓  \n",
      " ↓   ↓   ↓   →   ↓   ↓   ↓   ↓  \n",
      " ↓   ↓   ↓   H   →   →   ↓   ↓  \n",
      " ↓   →   ↓   ↓   ←   H   ↓   ↓  \n",
      " ↓   H   ↓   ↓   H   ↓   →   ↓  \n",
      " ↓   H   ↓   ↓   ↓   ↓   H   ↓  \n",
      " →   →   →   →   ↓   ↓   H   ↓  \n",
      " →   →   ↑   H   →   →   →   G  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(env, policy, episodes=100):\n",
    "    total_reward = 0\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[obs]\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        success += reward\n",
    "    return success / episodes, total_reward / episodes\n",
    "\n",
    "env = MyFrozenLakeEnv()\n",
    "\n",
    "vi_policy, vi_V = value_iteration(env)\n",
    "vi_success, vi_reward = evaluate_policy(env, vi_policy)\n",
    "print(\"Value Iteration\")\n",
    "print(\"Success Rate:\", vi_success)\n",
    "print(\"Avg Reward:\", vi_reward)\n",
    "print_policy_arrows(vi_policy, env)\n",
    "\n",
    "pi_policy, pi_V, pi_iters = policy_iteration(env)\n",
    "pi_success, pi_reward = evaluate_policy(env, pi_policy)\n",
    "print(\"Policy Iteration\")\n",
    "print(\"Success Rate:\", pi_success)\n",
    "print(\"Avg Reward:\", pi_reward)\n",
    "print(\"Iterations:\", pi_iters)\n",
    "print_policy_arrows(pi_policy, env)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1ea09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
