{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e21f487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_map_8x8 = [\n",
    "    list(\"SFFFFFFF\"),\n",
    "    list(\"HFFFHFFF\"),\n",
    "    list(\"FFFFFFFH\"),\n",
    "    list(\"FFFFFFFF\"),\n",
    "    list(\"FFFFFFFF\"),\n",
    "    list(\"FFFFFFFF\"),\n",
    "    list(\"FHFFHFFF\"),\n",
    "    list(\"FFFFFFFF\"),\n",
    "    list(\"FFFFFFFF\"),\n",
    "    list(\"FFFFFFFG\")\n",
    "]\n",
    "custom_map_50x50 = [\n",
    "    list(row) for row in [\n",
    "        \"SFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFHFFFFFFFFFFFHFFF\",\n",
    "        \"FHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFF\",\n",
    "        \"FHFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFFFFFFHFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFFFFFF\",\n",
    "        \"FFHFHFFFFFFFHFFFHFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFF\",\n",
    "        \"FFFFFFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFHFHFFFFFFFFHF\",\n",
    "        \"HFFFFFHFFFFFFFFFFFFFFFFHHHFFFFFFFFHFFFHFFFFFFFFFFF\",\n",
    "        \"FFFFFHFFFHFFFFFFFFFFFFHHFFFFFFFFFFFFFFFFFHFFHFFHHF\",\n",
    "        \"FFFFFFFFFFFFHFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFF\",\n",
    "        \"FFFFFFFFFFFFHFFFFFFFFFFFFHFFFFFFFFFFFFFFFFFFFFHFFF\",\n",
    "        \"FFFFFFFFFFHFFFFFHFFFFFHFFFHFFFFFFFFFFFFFFFFHFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFHFFFFFFFFFFFFFFFHFFFFFHHFFFHFFFHFFFFFF\",\n",
    "        \"FFHFFFFHFFFFFFFFFFFFFFHFFFFFFFFFFFFFFFFFHHFFFFFFHF\",\n",
    "        \"FHFFFFHFFFHFFFFFHFFHFHFFFFFFFFFHFFFFFFHFFFFFHFFFFF\",\n",
    "        \"FFFFFFFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHHFFF\",\n",
    "        \"FFFFFFFHFFFFFFFHFFFFFFFFFHFHHFFFFFFFFFHFFFFFFFFFFF\",\n",
    "        \"HFFFFFFFFHFFFFFHFFFFFFHFHFFFFFFFHHFFFFHFHFHFFHFFFF\",\n",
    "        \"FFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFHFFFFFFFFFFF\",\n",
    "        \"FFFFFHHFFFFFFFFFFFFFFFFHFFFFFFFFFFFFFFFFFFFFHFFFFH\",\n",
    "        \"FFFFFFFFHFFFFFFHFFFFFFFFFFFFFFFFFFFFHFFFFFHFFFFHFH\",\n",
    "        \"FFFHFFFFFFFFHFFFFHFFFFFFFFFFFFFFFFFFHFFFFFFFFFFHFF\",\n",
    "        \"FFFFFFFFFFFFFFHFHFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFFF\",\n",
    "        \"FFFFFFFFFHFFFFFFFFHHFFFFHFFFFFFHHFHFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFHFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFHFFFFFF\",\n",
    "        \"FFHFFHHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFHFFFHFFFFFFFFFFFHFFFFFFHFFHFFFFFFFFFFFFFFFFHFF\",\n",
    "        \"HFFFFFFFFFHHFHFFFFFFFFHFFFFFFFFFFFFFFFFFHFFFFFFFFF\",\n",
    "        \"FFHFFFFFFFFFHFFHFHFFHFFFFFFHFFFFFFFFHFFFFFFFFFFFFF\",\n",
    "        \"HFFFFFFFFFFFFFFFFFFFFHFFHHFFHFFFFFHFFFFFHFFFFFFHFF\",\n",
    "        \"FFFFHFFFHFFFFFFFFHFFHFFFFFFFFFFFFFFFFFFFFFFFFHFFFF\",\n",
    "        \"FFFFFFFHFFFFFFFFFFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFHFHFFFFFFHFHFFFFFFFHHFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFHFFFFHFFFFFFFFFFFFFFHHFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFHFFFFFFFFFFFFFHFFFHFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFHFHFFFFHFHFFFFFFFFFFHFFFFFFFFFFFFFFFFFFFFHFF\",\n",
    "        \"HFFFFFFFFFFFFFFFFFFFFFFHFFFHFFFHHHFFHFFFFFFFHFFFFF\",\n",
    "        \"FFFFFFFFFFFFHFFHFFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFHFFFFFFFFFFFFFFFHFFFFFFFFFFFFFHFFFF\",\n",
    "        \"FFHHFFFHFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFHFHFFHFFFF\",\n",
    "        \"FFFFFHFFFFFFHFFFHHFFHHFFFHFFFFFHHFFFFHFFHFFFFFFFFH\",\n",
    "        \"FFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFHFFFFFHFHFFFFFFFFFFFHFFFFFHFFFFFFFFFFFHFFFFFFFFF\",\n",
    "        \"FFFFFFHFFHFHHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFFFFFFFFFFHFFFFFHHF\",\n",
    "        \"FFHFFFHHFFFFHHFHHFFFFFFFFFFFFFHFFFFFFFFFFFFFFHFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFHHFFFFFFFFFFFFH\",\n",
    "        \"FFFFFFFFFHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFH\",\n",
    "        \"FHFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFFHFFFFG\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import time\n",
    "\n",
    "class CustomFrozenLakeEnv(gym.Env):\n",
    "    def __init__(self, grid, slip_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.map = grid\n",
    "        self.nrow = len(self.map)\n",
    "        self.ncol = len(self.map[0])\n",
    "        self.nS = self.nrow * self.ncol\n",
    "        self.nA = 4\n",
    "        self.slip_prob = slip_prob\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "        self.pos_to_state = lambda r, c: r * self.ncol + c\n",
    "        self.state_to_pos = lambda s: (s // self.ncol, s % self.ncol)\n",
    "        self.state = None\n",
    "        self.P = self._build_transition_matrix()\n",
    "\n",
    "    def _build_transition_matrix(self):\n",
    "        P = {s: {a: [] for a in range(self.nA)} for s in range(self.nS)}\n",
    "        for r in range(self.nrow):\n",
    "            for c in range(self.ncol):\n",
    "                s = self.pos_to_state(r, c)\n",
    "                tile = self.map[r][c]\n",
    "                for a in range(self.nA):\n",
    "                    transitions = []\n",
    "                    directions = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # L, D, R, U\n",
    "\n",
    "                    for i, (dr, dc) in enumerate(directions):\n",
    "                        prob = 1 - self.slip_prob if i == a else self.slip_prob / 3.0\n",
    "                        new_r = min(max(r + dr, 0), self.nrow - 1)\n",
    "                        new_c = min(max(c + dc, 0), self.ncol - 1)\n",
    "                        new_s = self.pos_to_state(new_r, new_c)\n",
    "                        new_tile = self.map[new_r][new_c]\n",
    "                        reward = 1.0 if new_tile == 'G' else -1.0 if new_tile == 'H' else -0.01\n",
    "                        done = new_tile in \"GH\"\n",
    "                        transitions.append((prob, new_s, reward, done))\n",
    "\n",
    "                    P[s][a] = transitions\n",
    "        return P\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = (0, 0)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        r, c = self.state\n",
    "        directions = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # L, D, R, U\n",
    "\n",
    "        if np.random.rand() < self.slip_prob:\n",
    "            action = np.random.choice([a for a in range(self.nA) if a != action])\n",
    "\n",
    "        dr, dc = directions[action]\n",
    "        r = min(max(r + dr, 0), self.nrow - 1)\n",
    "        c = min(max(c + dc, 0), self.ncol - 1)\n",
    "        self.state = (r, c)\n",
    "\n",
    "        tile = self.map[r][c]\n",
    "        reward = 1.0 if tile == 'G' else -1.0 if tile == 'H' else -0.01\n",
    "        done = tile in \"GH\"\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        r, c = self.state\n",
    "        return self.pos_to_state(r, c)\n",
    "    \n",
    "def evaluate_policy(env, policy, episodes=100):\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "    success_count = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            steps += 1\n",
    "            total_reward += reward\n",
    "            if reward == 1.0:  # goal reached\n",
    "                success_count += 1\n",
    "\n",
    "        total_steps += steps\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    avg_length = total_steps / episodes\n",
    "    success_rate = success_count / episodes\n",
    "\n",
    "    return  avg_length,avg_reward\n",
    "\n",
    "\n",
    "\n",
    "def run_and_evaluate(algo_name, algo_func, env, episodes, is_model_based=False):\n",
    "    start_time = time.time()\n",
    "    if is_model_based:\n",
    "        policy = algo_func(env)\n",
    "        iterations = episodes\n",
    "    else:\n",
    "        Q = algo_func(env, episodes)\n",
    "        policy = np.argmax(Q, axis=1)\n",
    "        iterations = episodes\n",
    "    duration = time.time() - start_time\n",
    "    avg_reward, avg_length = evaluate_policy(env, policy, 100)\n",
    "    print(f\"{algo_name:<20} {duration:.4f}     {avg_length:.3f}         {avg_reward:.3f}   {iterations}\")\n",
    "    return policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23308a13",
   "metadata": {},
   "source": [
    "helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def greedy_action(Q_s):\n",
    "    return np.random.choice(np.flatnonzero(Q_s == Q_s.max()))\n",
    "\n",
    "def get_policy(Q):\n",
    "    return np.argmax(Q, axis=1)\n",
    "\n",
    "def evaluate_policy(env, policy, episodes=1000):\n",
    "    total_reward = 0\n",
    "    total_length = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            total_length += 1\n",
    "    return total_length / episodes, total_reward / episodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa26cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env, episodes, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    N = np.zeros((env.nS, env.nA))\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample() if np.random.rand() < epsilon else greedy_action(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in visited:\n",
    "                visited.add((s, a))\n",
    "                N[s, a] += 1\n",
    "                Q[s, a] += (G - Q[s, a]) / N[s, a]\n",
    "    return Q\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "073ed698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, episodes, gamma=0.99):\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    epsilon, alpha = 0.5, 0.5\n",
    "    for ep in range(episodes):\n",
    "        decay = 1 - ep / episodes\n",
    "        e = epsilon * decay\n",
    "        a = alpha * decay\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample() if np.random.rand() < e else greedy_action(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            Q[state, action] += a * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            state = next_state\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e0c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, episodes, gamma=0.99):\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    epsilon, alpha = 0.5, 0.5\n",
    "    for ep in range(episodes):\n",
    "        decay = 1 - ep / episodes\n",
    "        e = epsilon * decay\n",
    "        a = alpha * decay\n",
    "        state, _ = env.reset()\n",
    "        action = env.action_space.sample() if np.random.rand() < e else greedy_action(Q[state])\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_action = env.action_space.sample() if np.random.rand() < e else greedy_action(Q[next_state])\n",
    "            Q[state, action] += a * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "            state, action = next_state, next_action\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44891b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-5):\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    V = np.zeros(env.nS)\n",
    "    is_stable = False\n",
    "    P = env.P\n",
    "\n",
    "    while not is_stable:\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.nS):\n",
    "                v = V[s]\n",
    "                a = policy[s]\n",
    "                V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r, _ in P[s][a]])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        \n",
    "        is_stable = True\n",
    "        for s in range(env.nS):\n",
    "            old_action = policy[s]\n",
    "            q_vals = np.array([sum([p * (r + gamma * V[s_]) for p, s_, r, _ in P[s][a]]) for a in range(env.nA)])\n",
    "            policy[s] = np.argmax(q_vals)\n",
    "            if old_action != policy[s]:\n",
    "                is_stable = False\n",
    "    return policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d285fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-5):\n",
    "    V = np.zeros(env.nS)\n",
    "    P = env.P\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max([sum([p * (r + gamma * V[s_]) for p, s_, r, _ in P[s][a]]) for a in range(env.nA)])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        q_vals = np.array([sum([p * (r + gamma * V[s_]) for p, s_, r, _ in P[s][a]]) for a in range(env.nA)])\n",
    "        policy[s] = np.argmax(q_vals)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm            Time (s)    Avg Reward   Avg Ep Len   Episodes\n",
      "----------------------------------------------------------------------\n",
      "Monte Carlo          9.6781     0.483         26.700   40000\n",
      "SARSA                0.4597     0.639         21.080   2000\n",
      "Q-learning           0.5138     0.631         21.910   2000\n",
      "Policy Iteration     0.2494     0.340         16.950   14\n",
      "Value Iteration      0.3088     0.542         18.830   14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{'Algorithm':<20} {'Time (s)':<10}  {'Avg Reward':<12} {'Avg Ep Len':<12} {'Episodes'}\")\n",
    "print(\"-\" * 70)\n",
    "env = CustomFrozenLakeEnv(custom_map_8x8, slip_prob=0.2)\n",
    "run_and_evaluate(\"Monte Carlo\", monte_carlo, env, episodes=40000)\n",
    "run_and_evaluate(\"SARSA\", sarsa, env, episodes=2000)\n",
    "run_and_evaluate(\"Q-learning\", q_learning, env, episodes=2000)\n",
    "run_and_evaluate(\"Policy Iteration\", policy_iteration, env, episodes=14, is_model_based=True)\n",
    "run_and_evaluate(\"Value Iteration\", value_iteration, env, episodes=14, is_model_based=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749607db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm            Time (s)    Avg Reward   Avg Ep Len   Episodes\n",
      "----------------------------------------------------------------------\n",
      "Monte Carlo          4.5584     -1.379         38.860   40000\n",
      "SARSA                1.3494     -2.154         116.410   2000\n",
      "Q-learning           1.4390     -3.095         210.520   2000\n",
      "Policy Iteration     17.6725     -1.097         102.680   14\n",
      "Value Iteration      9.6555     -1.099         100.850   14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{'Algorithm':<20} {'Time (s)':<10}  {'Avg Reward':<12} {'Avg Ep Len':<12} {'Episodes'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "env = CustomFrozenLakeEnv(custom_map_50x50, slip_prob=0.2)\n",
    "\n",
    "run_and_evaluate(\"Monte Carlo\", monte_carlo, env, episodes=40000)\n",
    "run_and_evaluate(\"SARSA\", sarsa, env, episodes=2000)\n",
    "run_and_evaluate(\"Q-learning\", q_learning, env, episodes=2000)\n",
    "run_and_evaluate(\"Policy Iteration\", policy_iteration, env, episodes=14, is_model_based=True)\n",
    "run_and_evaluate(\"Value Iteration\", value_iteration, env, episodes=14, is_model_based=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
